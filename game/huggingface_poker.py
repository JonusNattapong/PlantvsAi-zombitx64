import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup
from torch.utils.data import DataLoader, Dataset, random_split
from datasets import load_dataset
import json
import os
from torch import nn
import datetime
import numpy as np
from tqdm import tqdm
import time
import multiprocessing
from torch.cuda.amp import autocast, GradScaler

# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô CPU cores ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ
num_workers = max(1, multiprocessing.cpu_count() - 1)

# Dataset class ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û
class PokerDataset(Dataset):
    def __init__(self, texts, features, labels, tokenizer, max_length=64):
        self.texts = texts
        self.features = features
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = self.texts[idx]
        features = self.features[idx]
        label = self.labels[idx]
        
        # Pre-tokenize
        encoding = self.tokenizer(
            text,
            padding='max_length',
            truncation=True,
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        # ‡πÅ‡∏õ‡∏•‡∏á tensor shape
        input_ids = encoding['input_ids'].squeeze()
        attention_mask = encoding['attention_mask'].squeeze()
        
        # ‡πÅ‡∏õ‡∏•‡∏á features ‡πÄ‡∏õ‡πá‡∏ô tensor
        feature_tensor = torch.tensor([
            features['player_rank_sum'], 
            features['ai_rank_sum'],
            features['community_rank_sum'],
            int(features['is_suited'])
        ], dtype=torch.float)
        
        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'features': feature_tensor,
            'labels': torch.tensor(label)
        }

# 1. ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡∏∞‡πÇ‡∏ó‡πÄ‡∏Ñ‡πá‡∏ô‡∏ô‡∏¥‡πÄ‡∏ã‡∏≠‡∏£‡πå
def load_model():
    """‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏≤‡∏Å Hugging Face ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á"""
    model_name = "bert-base-uncased"
    
    # ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏ó‡πÄ‡∏Ñ‡πá‡∏ô‡∏ô‡∏¥‡πÄ‡∏ã‡∏≠‡∏£‡πå
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    
    # ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á
    class AdvancedPokerModel(nn.Module):
        def __init__(self, model_name, num_labels):
            super().__init__()
            # ‡πÇ‡∏´‡∏•‡∏î BERT ‡πÅ‡∏ö‡∏ö‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á
            self.bert = AutoModelForSequenceClassification.from_pretrained(
                model_name,
                num_labels=num_labels,
                ignore_mismatched_sizes=True
            )
            
            # ‡πÄ‡∏û‡∏¥‡πà‡∏° layers ‡∏û‡∏¥‡πÄ‡∏®‡∏©
            self.dropout1 = nn.Dropout(0.3)
            self.feature_layer = nn.Linear(4, 64)  # 4 features -> 64 units
            self.activation = nn.ReLU()
            
            # Layer ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á BERT ‡∏Å‡∏±‡∏ö features
            self.dropout2 = nn.Dropout(0.2)
            self.combined_layer = nn.Linear(768 + 64, 256)
            
            # Output layer
            self.classifier = nn.Linear(256, num_labels)
            
            # Batch normalization ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏ó‡∏£‡∏ô‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô
            self.batch_norm1 = nn.BatchNorm1d(64)
            self.batch_norm2 = nn.BatchNorm1d(256)
        
        def forward(self, input_ids=None, attention_mask=None, features=None, labels=None):
            # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• text ‡∏î‡πâ‡∏ß‡∏¢ BERT
            bert_outputs = self.bert.bert(input_ids=input_ids, attention_mask=attention_mask)
            pooled_output = bert_outputs.pooler_output  # [batch_size, 768]
            pooled_output = self.dropout1(pooled_output)
            
            # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• features
            feature_output = self.feature_layer(features)  # [batch_size, 64]
            feature_output = self.activation(feature_output)
            feature_output = self.batch_norm1(feature_output)
            
            # ‡∏£‡∏ß‡∏° text features ‡∏Å‡∏±‡∏ö numerical features
            combined = torch.cat((pooled_output, feature_output), dim=1)  # [batch_size, 768+64]
            combined = self.dropout2(combined)
            
            # ‡∏™‡πà‡∏á‡∏ú‡πà‡∏≤‡∏ô combined layer
            hidden = self.combined_layer(combined)  # [batch_size, 256]
            hidden = self.activation(hidden)
            hidden = self.batch_norm2(hidden)
            
            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì logits
            logits = self.classifier(hidden)  # [batch_size, num_labels]
            
            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì loss ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ labels
            loss = None
            if labels is not None:
                loss_fct = nn.CrossEntropyLoss()
                loss = loss_fct(logits.view(-1, 3), labels.view(-1))
            
            return {"loss": loss, "logits": logits}

    model = AdvancedPokerModel(model_name, 3)
    return tokenizer, model

# 2. ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ
def prepare_dataset():
    """‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å poker_dataset.json ‡πÉ‡∏´‡πâ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô"""
    dataset_path = os.path.join('DatasetPokerzombitx64', 'poker_dataset.json')
    with open(dataset_path, 'r') as f:
        data = json.load(f)
    
    # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Dataset class
    texts = []
    features_list = []
    labels = []
    
    # ‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡πà‡∏≤ rank ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç
    rank_values = {'2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, 
                  '8': 8, '9': 9, '10': 10, 'J': 11, 'Q': 12, 'K': 13, 'A': 14}
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á dataset ‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
    for game in data:
        # ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
        text = f"Player cards: {game['player_cards'][0]['rank']} of {game['player_cards'][0]['suit']}, {game['player_cards'][1]['rank']} of {game['player_cards'][1]['suit']}"
        text += f" AI cards: {game['ai_cards'][0]['rank']} of {game['ai_cards'][0]['suit']}, {game['ai_cards'][1]['rank']} of {game['ai_cards'][1]['suit']}"
        text += f" Community: {game['community_cards'][0]['rank']} of {game['community_cards'][0]['suit']}, {game['community_cards'][1]['rank']} of {game['community_cards'][1]['suit']}"
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏° feature ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°
        features = {
            'player_rank_sum': sum([rank_values[game['player_cards'][0]['rank']], rank_values[game['player_cards'][1]['rank']]]),
            'ai_rank_sum': sum([rank_values[game['ai_cards'][0]['rank']], rank_values[game['ai_cards'][1]['rank']]]),
            'community_rank_sum': sum([rank_values[game['community_cards'][0]['rank']], rank_values[game['community_cards'][1]['rank']]]),
            'is_suited': game['player_cards'][0]['suit'] == game['player_cards'][1]['suit']
        }
        
        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î label
        if game['winner'] == 'player':
            label = 0
        elif game['winner'] == 'ai':
            label = 1
        else:
            label = 2
        
        texts.append(text)
        features_list.append(features)
        labels.append(label)
    
    return texts, features_list, labels

# 3. ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•
def train_model(tokenizer, model, dataset_data):
    """‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏û‡∏¥‡πÄ‡∏®‡∏©‡πÅ‡∏•‡∏∞ checkpoint"""
    texts, features_list, labels = dataset_data
    
    # ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô train/validation
    total_size = len(texts)
    train_size = int(0.9 * total_size)
    val_size = total_size - train_size
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á indices ‡πÅ‡∏ö‡∏ö‡∏™‡∏∏‡πà‡∏°
    indices = list(range(total_size))
    np.random.shuffle(indices)
    train_indices = indices[:train_size]
    val_indices = indices[train_size:]
    
    # ‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• train/val
    train_texts = [texts[i] for i in train_indices]
    train_features = [features_list[i] for i in train_indices]
    train_labels = [labels[i] for i in train_indices]
    
    val_texts = [texts[i] for i in val_indices]
    val_features = [features_list[i] for i in val_indices]
    val_labels = [labels[i] for i in val_indices]
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á Dataset objects
    train_dataset = PokerDataset(train_texts, train_features, train_labels, tokenizer)
    val_dataset = PokerDataset(val_texts, val_features, val_labels, tokenizer)
    
    # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ hyperparameters
    batch_size = 64  # ‡πÄ‡∏û‡∏¥‡πà‡∏° batch size
    epochs = 5
    accumulation_steps = 4  # ‡πÉ‡∏ä‡πâ gradient accumulation
    
    # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î DataLoader
    train_loader = DataLoader(
        train_dataset, 
        batch_size=batch_size, 
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True
    )
    
    val_loader = DataLoader(
        val_dataset, 
        batch_size=batch_size,
        num_workers=num_workers,
        pin_memory=True
    )
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á checkpoint directory
    checkpoint_dir = os.path.join('DatasetPokerzombitx64', 'checkpoints')
    os.makedirs(checkpoint_dir, exist_ok=True)
    
    # ‡πÇ‡∏´‡∏•‡∏î checkpoint ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
    checkpoint_path = os.path.join(checkpoint_dir, 'latest_checkpoint.pth')
    start_epoch = 0
    
    # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ optimizer
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=2e-5,  # ‡∏õ‡∏£‡∏±‡∏ö learning rate
        betas=(0.9, 0.999),
        eps=1e-8,
        weight_decay=0.01
    )
    
    # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ learning rate scheduler ‡∏û‡∏£‡πâ‡∏≠‡∏° warmup
    total_steps = len(train_loader) * epochs // accumulation_steps
    warmup_steps = int(total_steps * 0.1)  # 10% warmup
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=warmup_steps,
        num_training_steps=total_steps
    )
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á gradient scaler ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö mixed precision
    scaler = GradScaler()
    
    if os.path.exists(checkpoint_path):
        print("Loading checkpoint...")
        checkpoint = torch.load(checkpoint_path)
        model.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        scaler.load_state_dict(checkpoint['scaler_state_dict'])
        start_epoch = checkpoint['epoch'] + 1
        print(f"‚úÖ Resuming training from epoch {start_epoch}")
    
    # ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥
    def compute_accuracy(predictions, labels):
        return (predictions == labels).float().mean().item()
    
    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
    start_time = time.time()
    
    # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô
    for epoch in range(start_epoch, epochs):
        model.train()
        train_loss = 0.0
        train_acc = 0.0
        
        # ‡πÅ‡∏™‡∏î‡∏á progress bar
        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs} [Train]")
        optimizer.zero_grad()
        
        # ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡πÄ‡∏ó‡∏£‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ batch
        for step, batch in enumerate(pbar):
            # ‡πÇ‡∏¢‡∏ô batch ‡πÑ‡∏õ‡∏ó‡∏µ‡πà GPU ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
            batch = {k: v for k, v in batch.items()}
            
            # ‡πÉ‡∏ä‡πâ mixed precision training
            with autocast():
                outputs = model(
                    input_ids=batch['input_ids'],
                    attention_mask=batch['attention_mask'],
                    features=batch['features'],
                    labels=batch['labels']
                )
                
                loss = outputs['loss']
                # ‡∏´‡∏≤‡∏£ loss ‡∏î‡πâ‡∏ß‡∏¢‡∏à‡∏≥‡∏ô‡∏ß‡∏ô accumulation steps
                loss = loss / accumulation_steps
            
            # Backward pass ‡∏î‡πâ‡∏ß‡∏¢ gradient scaling
            scaler.scale(loss).backward()
            
            # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï parameters ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏Ñ‡∏£‡∏ö accumulation steps
            if (step + 1) % accumulation_steps == 0:
                # Gradient clipping ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô exploding gradients
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                
                # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï parameters
                scaler.step(optimizer)
                scaler.update()
                scheduler.step()
                optimizer.zero_grad()
            
            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì metrics
            train_loss += loss.item() * accumulation_steps
            predictions = torch.argmax(outputs['logits'], dim=-1)
            accuracy = compute_accuracy(predictions, batch['labels'])
            train_acc += accuracy
            
            # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï progress bar
            pbar.set_postfix({
                'loss': f"{train_loss/(step+1):.4f}",
                'acc': f"{train_acc/(step+1):.4f}"
            })
            
            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å checkpoint ‡∏ó‡∏∏‡∏Å 50 batches
            if (step + 1) % 50 == 0:
                checkpoint = {
                    'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'scheduler_state_dict': scheduler.state_dict(),
                    'scaler_state_dict': scaler.state_dict(),
                    'loss': train_loss / (step + 1)
                }
                torch.save(checkpoint, checkpoint_path)
                print(f"‚è±Ô∏è Checkpoint saved at epoch {epoch + 1}, batch {step + 1}")
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì average metrics
        avg_train_loss = train_loss / len(train_loader)
        avg_train_acc = train_acc / len(train_loader)
        
        # ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ö‡∏ô validation set
        model.eval()
        val_loss = 0.0
        val_acc = 0.0
        
        with torch.no_grad():
            for batch in tqdm(val_loader, desc=f"Epoch {epoch+1}/{epochs} [Validation]"):
                batch = {k: v for k, v in batch.items()}
                
                # Inference ‡πÉ‡∏ô‡πÇ‡∏´‡∏°‡∏î evaluation
                outputs = model(
                    input_ids=batch['input_ids'],
                    attention_mask=batch['attention_mask'],
                    features=batch['features'],
                    labels=batch['labels']
                )
                
                loss = outputs['loss']
                val_loss += loss.item()
                
                predictions = torch.argmax(outputs['logits'], dim=-1)
                accuracy = compute_accuracy(predictions, batch['labels'])
                val_acc += accuracy
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì average validation metrics
        avg_val_loss = val_loss / len(val_loader)
        avg_val_acc = val_acc / len(val_loader)
        
        # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
        print(f"Epoch {epoch+1}/{epochs}")
        print(f"Train Loss: {avg_train_loss:.4f} | Train Acc: {avg_train_acc:.4f}")
        print(f"Val Loss: {avg_val_loss:.4f} | Val Acc: {avg_val_acc:.4f}")
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏à‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ epoch
        model_path = os.path.join(checkpoint_dir, f'model_epoch_{epoch + 1}.pth')
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict(),
            'scaler_state_dict': scaler.state_dict(),
            'train_loss': avg_train_loss,
            'val_loss': avg_val_loss,
            'train_acc': avg_train_acc,
            'val_acc': avg_val_acc
        }, model_path)
        print(f"üîÑ Model saved for epoch {epoch + 1}")
    
    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
    total_time = time.time() - start_time
    print(f"‚ö° Total training time: {total_time:.2f} seconds")
    
    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
    model.bert.save_pretrained('DatasetPokerzombitx64/huggingface_model')
    tokenizer.save_pretrained('DatasetPokerzombitx64/huggingface_model')
    
    print("üéâ Model training completed and saved")
    
    return model

# 4. ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
def predict(tokenizer, model, player_cards, ai_cards, community_cards):
    """‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡πà‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏û‡∏¥‡πÄ‡∏®‡∏©"""
    # ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
    text = f"Player cards: {player_cards[0]['rank']} of {player_cards[0]['suit']}, {player_cards[1]['rank']} of {player_cards[1]['suit']}"
    text += f" AI cards: {ai_cards[0]['rank']} of {ai_cards[0]['suit']}, {ai_cards[1]['rank']} of {ai_cards[1]['suit']}"
    text += f" Community: {community_cards[0]['rank']} of {community_cards[0]['suit']}, {community_cards[1]['rank']} of {community_cards[1]['suit']}"
    
    # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° features
    rank_values = {'2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, 
                  '8': 8, '9': 9, '10': 10, 'J': 11, 'Q': 12, 'K': 13, 'A': 14}
    
    features = {
        'player_rank_sum': sum([rank_values[player_cards[0]['rank']], rank_values[player_cards[1]['rank']]]),
        'ai_rank_sum': sum([rank_values[ai_cards[0]['rank']], rank_values[ai_cards[1]['rank']]]),
        'community_rank_sum': sum([rank_values[community_cards[0]['rank']], rank_values[community_cards[1]['rank']]]),
        'is_suited': player_cards[0]['suit'] == player_cards[1]['suit']
    }
    
    # ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô tensor
    inputs = tokenizer(text, return_tensors='pt')
    feature_tensor = torch.tensor([
        features['player_rank_sum'], 
        features['ai_rank_sum'],
        features['community_rank_sum'],
        int(features['is_suited'])
    ], dtype=torch.float).unsqueeze(0)  # Add batch dimension
    
    # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•
    model.eval()
    with torch.no_grad():
        outputs = model(
            input_ids=inputs['input_ids'],
            attention_mask=inputs['attention_mask'],
            features=feature_tensor
        )
        predictions = torch.argmax(outputs['logits'], dim=-1)
    
    # ‡πÅ‡∏õ‡∏•‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
    results = {
        0: "Player wins",
        1: "AI wins",
        2: "Draw"
    }
    
    return results[predictions.item()]

if __name__ == "__main__":
    print("üöÄ Starting advanced poker model training...")
    
    # ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•
    print("üìö Loading model and tokenizer...")
    tokenizer, model = load_model()
    
    # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    print("üîÑ Preparing dataset...")
    dataset_data = prepare_dataset()
    print(f"‚úÖ Dataset prepared with {len(dataset_data[0])} samples")
    
    # ‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•
    print("‚öôÔ∏è Starting training with advanced techniques...")
    model = train_model(tokenizer, model, dataset_data)
    
    # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
    print("üéØ Testing prediction...")
    test_player_cards = [{'rank': 'A', 'suit': 'Hearts'}, {'rank': 'K', 'suit': 'Diamonds'}]
    test_ai_cards = [{'rank': 'Q', 'suit': 'Clubs'}, {'rank': 'J', 'suit': 'Spades'}]
    test_community_cards = [{'rank': '10', 'suit': 'Hearts'}, {'rank': '9', 'suit': 'Diamonds'}]
    
    result = predict(tokenizer, model, test_player_cards, test_ai_cards, test_community_cards)
    print(f"üéÆ Prediction: {result}")
